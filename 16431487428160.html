<!DOCTYPE html>
<html lang="zh">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
    Machine Learning Lecture 02 Linear Regression and Gradient Descent - Yuanye Chi's Blog
    
    </title>
    <link rel="shortcut icon" href="https://i.loli.net/2019/02/20/5c6cf0555ffe6.jpeg" type="image/png" />

    
    
    <link href="atom.xml" rel="alternate" title="Yuanye Chi's Blog" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/style.min.css">
    <link rel="stylesheet" href="asset/css/doc.css">
    <script src="asset/app.js"></script>
</head>
  <body>
    <section class="hero">
      <div class="hero-head">
          <nav class="navbar" role="navigation" aria-label="main navigation">
              <div class="container">
              <div class="navbar-brand">
                
                <a target="_self" class="navbar-item " href="index.html">Home</a>
                
                <a target="_self" class="navbar-item " href="archives.html">Archives</a>
                
                <a target="_self" class="navbar-item " href="about.html">About</a>
                
                <a target="_self" class="navbar-item " href="https://chiyuanye-com-old-site.onrender.com/">OldSite</a>
                

                <a role="button" id="navbarSNSRssSwitchBtn" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navbarSNSRssButtons">
                  <span aria-hidden="true"></span>
                  <span aria-hidden="true"></span>
                  <span aria-hidden="true"></span>
                </a>
              </div>
            
              <div id="navbarSNSRssButtons" class="navbar-menu">
                <div class="navbar-start">
                  
                </div>
            
                <div class="navbar-end">
                  <div class="navbar-item">
                    <!--buttons start-->
                    <div class="buttons">
                      
                        
                        
                        
                        <a href="https://github.com/tsiye" target="_blank" title="github">
                            <span class="icon is-large has-text-grey-darker">
                               <svg class="svg-inline--fa fa-github fa-w-16 fa-lg" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github fa-lg"></i> -->
                            </span>
                          </a>
                        
                        
                      
                      <a href="atom.xml" target="_blank" title="RSS">
                          <span class="icon is-large has-text-black-bis">
                              <svg class="svg-inline--fa fa-rss fa-w-14 fa-lg" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="rss" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" data-fa-i2svg=""><path fill="currentColor" d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg><!-- <i class="fas fa-rss fa-lg"></i> -->
                          </span>
                      </a>
                    </div>
                    <!--buttons end-->

                  </div>
                </div>
                </div>
              </div>
            </nav>
      </div>

 <div class="hero-body ct-body"></div>
      
    </section>
    <section class="ct-body">
      <div class="container">
          <div class="columns is-variable bd-klmn-columns is-4 is-centered">
              <div class="column is-four-fifths">
                  <div class="post-body single-content">
                    
                    <h1 class="title">
                            Machine Learning Lecture 02 Linear Regression and Gradient Descent   
                      </h1>
                     
                    
                      <div class="media">
                            
                            <figure class="media-left">
                              <p class="image is-48x48">
                                
                                  <img class="is-rounded" src="https://i.loli.net/2019/02/20/5c6cf0e7a42f9.jpg">
                                
                              </p>
                            </figure>
                            
                            <div class="media-content">
                              <div class="content">
                                <p>
                                 <span class="date">2022/01/25 17:12 PM</span>
                                  <span class="tran-posted-in">posted in</span>&nbsp; 
                                  
                                      <span class="posted-in"><a href='Stanford%20CS229.html'>Stanford CS229</a></span>
                                         
                                  
                                    &nbsp;&nbsp;<a href="16431487428160.html#disqus_thread"><span class="tran-disqus-comments">comments</span></a>
                                  

                                   
                                      
                                  <br />
                                  <span class="tran-tags">Tags:</span>&nbsp;
                                  
                                    <a class="tag is-link is-light" href='tag_Stanford%20CS229.html'>#Stanford CS229</a>
                                     

                                </p>
                              </div>
                            </div>
                         
                    </div>
                </div>
                  <article class="markdown-body single-content">
                    <h2><a id="linear-regression" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Regression</h2>
<blockquote>
<p>The simplest algorithm in regression.</p>
</blockquote>
<h3><a id="example-of-self-driving" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Example of self-driving</h3>
<p>X: picture of what's in front of the car  ---map to----&gt; Y: Steering Direction</p>
<h3><a id="come-back-to-the-house-price-example" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Come back to the house price example</h3>
<table>
<thead>
<tr>
<th>Size</th>
<th style="text-align: left">Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>2104</td>
<td style="text-align: left">400</td>
</tr>
<tr>
<td>1416</td>
<td style="text-align: left">232</td>
</tr>
<tr>
<td>1534</td>
<td style="text-align: left">315</td>
</tr>
<tr>
<td>852</td>
<td style="text-align: left">178</td>
</tr>
</tbody>
</table>
<p>What we will do is fit a straight line to the data.</p>
<p>The procedure of supervise learning is <em>we apply the training set to learning algorithm, then output a function to make predictions about housing prices.</em>  The function is called <em>hypothesis.</em> When we input the size of the house, it will output the price.<br />
<img src="media/16431487428160/16431502968719.jpg" alt="" class="mw_img_center" style="width:500px;display: block; clear:both; margin: 0 auto;" /></p>
<p>The first thing when design an algorithm:</p>
<ol>
<li>How to represent the hypothesis, H?
<ul>
<li>In linear regression, the hypothesis should be \(h(x) = \theta_0 + \theta_1x\)(affine function)(one input feature)</li>
<li>If we continue to take number of bedrooms into account, then we will have \(x_2\). \(h(x) = \theta_0 + \theta_1x_1 + \theta_2x_2\)</li>
<li>In order to make the notation more compact, we can write as this,</li>
</ul>
\[h(x)=\sum_{j=0}^{2} \theta_{j} x_{j}, x_0 =  1
\]
\[ \theta=\left[\begin{array}{l}\theta_{0} \\ \theta_{1} \\ \theta_{2}\end{array}\right] \quad x=\left[\begin{array}{l}x_{0} \\ x_{1} \\ x_{0}\end{array}\right]
\]
<ul>
<li>\(\theta\) is called parameters</li>
<li>m is called number of training examples. (# rows in table above)</li>
<li>x denotes inputs/features.</li>
<li>y is the output/target variable</li>
<li>(x,y) is one training example.</li>
<li>\((x^{(i)},y^{(i)})\) is the ith training example.</li>
<li>\(x_1^{(1)}\) = 2104, \(x_1^{(2)}\) = 1416</li>
<li>n is  # features. Here n = 2.</li>
</ul>
</li>
<li>How to choose parameters \(\theta\)?
<ul>
<li>Choose \(\theta\) st.(such that) \(h(x)\approx y\) for training examples.</li>
<li>To clarify that h depends both an the x and parameter \(\theta\). We are going to use the denote \(h_\theta(x) = h(x)\).</li>
<li>We want to minimize the difference between \(h_\theta(x)-y\). Then we consider minimize\(\frac1 2\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2\). 1/2 is put by convention.</li>
<li><img src="media/16431487428160/16433028798578.jpg" alt="" class="mw_img_center" style="width:400px;display: block; clear:both; margin: 0 auto;" /></li>
<li>We define \(J(\theta)\) is equal to the minimize formula</li>
</ul>
</li>
</ol>
<h2><a id="gradient-descent" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient Descent</h2>
<blockquote>
<p>Start with some \(\theta\)(say \(\theta=\overrightarrow{0}\))<br />
Keep changing \(\theta\) to reduce \(J(\theta)\)</p>
</blockquote>
<p><img src="media/16431487428160/16433033015154.jpg" alt="" class="mw_img_center" style="width:400px;display: block; clear:both; margin: 0 auto;" /><br />
So the blue area are good point for us.</p>
<p>We start at a random place, and we look around to see if we could take a tiny little step, then in what direction should we take the little step?<br />
We will see teh steepest direction down hill.Then we will take another step.<br />
<img src="media/16431487428160/16433034778193.jpg" alt="" class="mw_img_center" style="width:400px;display: block; clear:both; margin: 0 auto;" /></p>
<p>But once we start from another point, we may get to another different local optimal.<br />
<img src="media/16431487428160/16433035187699.jpg" alt="" class="mw_img_center" style="width:400px;display: block; clear:both; margin: 0 auto;" /><br />
But in linear regression, we may not get into a local optimal state.</p>
<p>One step of gradient descent can be implemented as:</p>
\[\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta)
\]
<p>\((j = 0,1,2,...n) \) where n is the number of features.</p>
<ul>
<li>\(\alpha\) here is called learning rate.</li>
<li>the formula after \(\alpha\) is partial derivative of \(J(\theta)\), which tells the direction of the next step.</li>
<li>How to determine learning rate? According to practice, it is set to <strong>0.01</strong>.</li>
</ul>
<p>Use := to denote assignment. We are going to take the value on the right and then assign it to \(\theta\) on the left. (Just like i := i + 1, := is just like = in programming, and = is like == in programming)</p>
\[\begin{aligned}
&amp;\frac{\partial}{\partial \theta_{j}} J(\theta)\\
=&amp;2 \frac{1}{2}\left(h_{\theta}(x)-y\right)\cdot\frac{\partial}{\partial \theta_{j}}\left(h_{0}(x)-y\right) \\
=&amp;\left(h_{\theta}(x)-y\right) \cdot \frac{\partial}{\partial \theta_{j}}\left(\theta_{0} x_{0}+\theta_{1} x_1,+\cdots+\theta_{n} x_{n}-y)\right.\\
=&amp; (h_{\theta}(x)-y) \cdot x_j
\end{aligned}
\]
<p>So,</p>
\[\theta_{j}:=\theta_{j}-\alpha (h_{\theta}(x)-y) \cdot x_j
\]
<p>For multiple training examples,</p>
\[\theta_{j}:=\theta_{j}-\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y_{i}^{(i)}\right) \cdot x_{j}^{(i)}
\]
<p>Then the gradient descent algorithm is to <strong>repeat until convergence.</strong> In each iteration of gradient descent, do update for j = 0,1,...,n.</p>
<p>Perhaps we could find pretty good \(\theta\), it turns out \(J(\theta)\) will always look like this.<br />
<img src="media/16431487428160/16433112844708.jpg" alt="" class="mw_img_center" style="width:400px;display: block; clear:both; margin: 0 auto;" /><br />
The contours of the big bowl will be ellipsis.<br />
<img src="media/16431487428160/16433115053845.jpg" alt="" class="mw_img_center" style="width:400px;display: block; clear:both; margin: 0 auto;" /><br />
Because there is only one global minimum, We will eventually get there.</p>
<p>Now come to the choose of learning rate Alpha.</p>
<ul>
<li>If we set it too large, then the step will be too large, we will run past the minium.</li>
<li>If we set it too small, we need a lot of iterations and the algorithms will be slow.</li>
<li>Usually we will try a lot of values and see.</li>
<li>When we see the cost function increasing rather than decreasing, that's a very strong sign that the learning rate is</li>
</ul>
<h3><a id="a-real-dataset" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>A Real Dataset</h3>
<p><img src="media/16431487428160/16433118116404.jpg" alt="" class="mw_img_center" style="width:500px;display: block; clear:both; margin: 0 auto;" /><br />
The number of input features(m) is 49. If we set the \(\theta\) to zero, we mean that our straight line to fit to the data to be that horizontal line.<br />
<img src="media/16431487428160/16433118910829.jpg" alt="" class="mw_img_center" style="width:500px;display: block; clear:both; margin: 0 auto;" /><br />
Then we will find different parameter \(\theta\) that allows this straight line tot fit the data better. So here is a different values of \(\theta_0\) and \(\theta_1\) after one or two iteration.<br />
<img src="media/16431487428160/16433124840088.jpg" alt="" style="width:300px;" /><img src="media/16431487428160/16433125520959.jpg" alt="" style="width:300px;" /><br />
Here is the final descent result.<br />
<img src="media/16431487428160/16433126011761.jpg" alt="" class="mw_img_center" style="width:400px;display: block; clear:both; margin: 0 auto;" /></p>
<h3><a id="batch-gradient-descent" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batch Gradient Descent</h3>
<blockquote>
<p>The Batch means we look at the whole training example, which means the 49 features in this example.</p>
</blockquote>
<h4><a id="disadvantage" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Disadvantage</h4>
<p>When the dataset is too large, it is time-consuming to do the whole computing. Because in every step of iteration, we need to scan the whole database for information and do computing.</p>
<h3><a id="stochastic-gradient" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stochastic Gradient</h3>
<p>Repeat</p>
\[\begin{aligned}
&amp;\text { for } i =1 \text { to } m,\{ \\
&amp;\qquad \theta_{j}:=\theta_{j}+\alpha\left(y^{(i)}-h_{\theta}\left(x^{(i)}\right)\right) x_{j}^{(i)}\} \quad \text { (for every } j \text { ) }
\end{aligned}
\]
<p><img src="media/16431487428160/16433160662463.jpg" alt="" class="mw_img_center" style="width:400px;display: block; clear:both; margin: 0 auto;" />As this Gradient Descent just take one feature in each iteration, so it may act like a slightly random path. Eventually, it cannot optimally converge.</p>
<h3><a id="normal-equation" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normal Equation</h3>
<blockquote>
<p>There is a way to solve for the optimal value of the parameters theta to just jump in one step to the global optimum without needing to use an iterative algorithm.<br />
It works only for linear regression.</p>
</blockquote>
\[\begin{aligned}
&amp;\nabla_{\theta} \underbrace{J(\theta)}_{\theta \in \mathbb{R}^{n+1}}=\left[\begin{array}{l}
\frac{\partial J}{\partial \theta_{0}} \\
\frac{\partial J}{\partial \theta_{1}} \\
\frac{\partial J}{\partial \theta_{2}}
\end{array}\right]
\end{aligned}
\]
<p>Define A is a 2*2 Matrix.</p>
\[A \in \mathbb{R}^{2\times2} \quad A=\left[\begin{array}{ll}
A_{11} &amp; A_{12} \\
A_{21} &amp; A_{22}
\end{array}\right]
\]
<p>Lets say, f is a function takes in 2*2 matrix to real number.</p>
\[\begin{aligned}
&amp;f(A)=A_{11}+A_{12}^{2} \quad f: \mathbb{R}^{2 \times 2} \mapsto \mathbb{R} \\
&amp;f\left(\left[\begin{array}{ll}
5 &amp; 6 \\
7 &amp; 8
\end{array}\right]\right)=5+6^{2}
\end{aligned}
\]
\[\bigtriangledown_{A} f(A)=\left[\begin{array}{ll}
\frac{\partial f}{\partial A_{11}} &amp; \frac{\partial f}{\partial A_{12}} \\
\frac{\partial f}{\partial A_{21}} &amp; \frac{\partial f}{\partial A_{22}}
\end{array}\right]
\]
\[\bigtriangledown_{A} f(A)=\left[\begin{array}{cc}
1 &amp; 2 A_{12} \\
0 &amp; 0
\end{array}\right]
\]
<p>The broad outline we are going to do is: we are going to take</p>
\[\nabla_{\theta} J(\theta)=\overrightarrow{0}
\]
<p>If A is a square matrix.(A is \(\mathbb{R}^{n \times n} \))<br />
We denote the trace of A: tr A = sum of diagnoal entries =\(\sum_iA_{ii}\)<br />
Some properties of trace:</p>
<ol>
<li>\(tr A = trA^T\)</li>
<li>Define \(f(A) = tr(A B)\). Then \(\bigtriangledown_{A} f(A)=B^T\)</li>
<li>\(trAB = trBA\)</li>
<li>\(trABC = trCAB\)</li>
<li>\(\bigtriangledown_{A} trAA^TC = CA + C^TA\)</li>
</ol>
<p>Finally, we write down the J theta.</p>
\[J(\theta)=\frac{1}{2} \sum_{i=1}^{m}\left(h\left(x^{(i)}\right)-y^{(i)}\right)^{2}
\]
<p>If we define<br />
<img src="media/16431487428160/16433192245778.jpg" alt="" class="mw_img_center" style="width:300px;display: block; clear:both; margin: 0 auto;" /><br />
y be the label of trainning sample.<br />
<img src="media/16431487428160/16433195693299.jpg" alt="" class="mw_img_center" style="width:200px;display: block; clear:both; margin: 0 auto;" /><br />
Since \(h_\theta(x^{(i)}) = (x^{(i)})^T\theta\),<br />
<img src="media/16431487428160/16433196828799.jpg" alt="" class="mw_img_center" style="width:300px;display: block; clear:both; margin: 0 auto;" /></p>
<p>Also, we know \(z^{T} z=\sum_{i} z_{i}^{2}\).<br />
So,<br />
<img src="media/16431487428160/16433198786397.jpg" alt="" class="mw_img_center" style="width:400px;display: block; clear:both; margin: 0 auto;" /><br />
Now we will do</p>
<p><img src="media/16431487428160/16433200614032.jpg" alt="" class="mw_img_center" style="width:450px;display: block; clear:both; margin: 0 auto;" /></p>
<p>So \(X^TX\theta = X^Ty\) is called normal equations.</p>
<p>Then we can get \(\theta=(X^TX)^{-1}X^Ty\)</p>

                  </article>
                  <div class="comments-wrap">
                    <div class="share-comments">
                      

                      
                        <div id="disqus_thread"></div>
                      

                      
                    </div>
                  </div><!-- end comments wrap -->
              </div>
            </div><!-- end columns -->
      </div><!-- end container -->
    </section>



    <footer class="footer">
        <div class="content has-text-centered">
          <p>
              Copyright &copy; 2019
              Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
              Theme used <a target="_blank" href="https://bulma.io/">Bulma CSS</a>.
          </p>
        </div>
      </footer>



<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

  













<script src="asset/prism.js"></script>




<script type="text/javascript">
    var disqus_shortname = 'chiyuanye'; 

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>

<script type="text/javascript">
var disqus_shortname = 'chiyuanye'; 

(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = '//' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>
  
    




  </body>
</html>
